{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determine the kind of nonmarine sedimentary deposits that reflects arid environmental conditions\n",
      "Integrality of the Chern character in small codimension\n",
      "\n",
      "Abstract\n",
      "We prove an integrality property of the Chern character with values in Chow groups. As a consequence, we obtain a construction of the p-1 first homological Steenrod operations on Chow groups modulo p and p-primary torsion, over an arbitrary field. We provide applications to the study of correspondences between algebraic varieties.\n",
      "667 667\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "def getquestions():\n",
    "    # data location\n",
    "    question_dir = 'STEMquestions/data/'\n",
    "\n",
    "    # list to store the extracted questions\n",
    "    questions = []\n",
    "\n",
    "    # Walk through the base directory and its subfolders\n",
    "    for root, dirs, files in os.walk(question_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Open and read the JSON file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                    # Extract the 'Original question' and add to the list\n",
    "                    if \"Original question\" in data:\n",
    "                        questions.append(data[\"Original question\"])\n",
    "\n",
    "    return questions\n",
    "\n",
    "def getsentences(num_sentences):\n",
    "    # Ensure you have the Punkt tokenizer models for sentence splitting (if not done already)\n",
    "    # nltk.download('punkt')\n",
    "\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    # Define the base directory where your text files are located\n",
    "    base_dir = 'STEMtext/'\n",
    "\n",
    "    # Initialize a list to store the sentence chunks\n",
    "    sentence_chunks = []\n",
    "\n",
    "    # Number of sentences to extract\n",
    "    sentences_per_chunk = 3  # Number of sentences per chunk (adjust as needed)\n",
    "\n",
    "    # Loop through the text files in the directory\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Open and read the content of the text file\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                    # Tokenize the content into individual sentences\n",
    "                    sentences = sent_tokenize(content)\n",
    "                    \n",
    "                    # Group the sentences into chunks of sentences_per_chunk size\n",
    "                    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "                        chunk = \" \".join(sentences[i:i + sentences_per_chunk])\n",
    "                        sentence_chunks.append(chunk)\n",
    "                        \n",
    "                        # Stop if we have collected enough sentence chunks\n",
    "                        if len(sentence_chunks) >= num_sentences:\n",
    "                            break\n",
    "\n",
    "                # Stop outer loop if we have enough sentence chunks\n",
    "                if len(sentence_chunks) >= num_sentences:\n",
    "                    break\n",
    "\n",
    "    return sentence_chunks\n",
    "   \n",
    "\n",
    "questions = getquestions()\n",
    "sentences = getsentences(len(questions))\n",
    "print(questions[0])\n",
    "print(sentences[0])\n",
    "print(len(questions), len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode sentences as vectors and create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jkamp\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\purdueprep-TEZ28JzX-py3.12\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       original_text  \\\n",
      "0  Determine the kind of nonmarine sedimentary de...   \n",
      "1  What is the major source of chlorine in seawat...   \n",
      "2  Determine the use of actualism in geology to i...   \n",
      "3  Determine the formation of an ion from uncharg...   \n",
      "4  Determine the conditions that favor the preser...   \n",
      "\n",
      "                                                data  labels  \n",
      "0  [0.03205199912190437, 0.02383396029472351, 0.0...       1  \n",
      "1  [-0.03738083317875862, -0.014038288034498692, ...       1  \n",
      "2  [-0.08302020281553268, 0.08540865033864975, 0....       1  \n",
      "3  [-0.02038067951798439, 0.0908573791384697, -0....       1  \n",
      "4  [-0.10606085509061813, 0.10114321112632751, 0....       1  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "def vector_encode(text):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    encoded = model.encode(text)\n",
    "    return encoded.tolist()\n",
    "\n",
    "questions_encoded = vector_encode(questions)\n",
    "sentences_encoded = vector_encode(sentences)\n",
    "\n",
    "question_labels = [1] * len(questions_encoded) # model outputs 1 ('true') for exam questions\n",
    "sentence_labels = [0] * len(sentences_encoded)\n",
    "\n",
    "# Create a DataFrame with columns for text, encodings, and labels\n",
    "original_text = questions + sentences\n",
    "encoded_data = questions_encoded + sentences_encoded\n",
    "labels = question_labels + sentence_labels\n",
    "df = pd.DataFrame({'original_text': original_text, 'data': encoded_data, 'labels': labels})\n",
    "\n",
    "# Display the first few rows of the DataFrame (optional)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train K-nearest neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9925093632958801%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['data'].tolist(), df['labels'], test_size=0.2, random_state=42)\n",
    "# Initialize the K-Nearest Neighbors classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Train the KNN model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model with accuracy score\n",
    "count_same = 0\n",
    "for a, b in zip(y_pred, y_test):\n",
    "    if a == b:\n",
    "        count_same += 1\n",
    "print(f\"Model accuracy: {count_same / len(y_pred)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test on new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jkamp\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\purdueprep-TEZ28JzX-py3.12\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_sentences = getsentences(2000)\n",
    "test_sentences = test_sentences[667:]\n",
    "test_sentences_enc = vector_encode(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "0.5918979744936234\n"
     ]
    }
   ],
   "source": [
    "print(len(test_sentences_enc[0]))\n",
    "preds = knn.predict(test_sentences_enc)\n",
    "print(preds.tolist().count(1)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_sentences_enc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))  \u001b[38;5;66;03m# Check similarity between a train and test vector\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(X_train[0] - test_sentences_enc[0]))  # Check similarity between a train and test vector\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "purdueprep-TEZ28JzX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
